"""
rss_crawler.py - RSS è®¢é˜…æŠ“å–å·¥å…·

åŠŸèƒ½ï¼š
- ä» RSSHub ç­‰æºæŠ“å–æœ€æ–°æ›´æ–°ï¼ˆå¦‚ Twitter, YouTube, åšå®¢ï¼‰
- è°ƒç”¨ LLM å¯¹æŠ“å–å†…å®¹è¿›è¡Œç»“æ„åŒ–æ•´ç†

ä¾èµ–ï¼šfeedparser, openai, python-dateutil
"""
import os
import json
import time
import configparser
import feedparser
from datetime import datetime, timezone
from dateutil import parser as date_parser
from concurrent.futures import ThreadPoolExecutor, as_completed
from common import organize_single_post, group_posts_by_domain, save_batch_manifest, DAYS_LOOKBACK, setup_logger

logger = setup_logger("rss_crawler")
from content_fetcher import ContentFetcher

# ================= é…ç½®åŠ è½½ =================
# åŠ è½½é…ç½®æ–‡ä»¶ (config.iniï¼Œä½äºé¡¹ç›®æ ¹ç›®å½•)
config = configparser.ConfigParser()
config.optionxform = str  # ä¿ç•™ key çš„å¤§å°å†™
config.read(os.path.join(os.path.dirname(__file__), '..', 'config-test.ini'), encoding='utf-8')

def load_weixin_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = RSSåœ°å€
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    weixin_accounts = {}
    
    if config.has_section('weixin_accounts'):
        for display_name in config.options('weixin_accounts'):
            rss_url = config.get('weixin_accounts', display_name).strip()
            if rss_url:
                weixin_accounts[display_name] = rss_url
    
    return weixin_accounts

def load_x_accounts_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ X (Twitter) è´¦æˆ·åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = è´¦æˆ·ID
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    x_accounts = {}
    rsshub_base_url = config.get('rsshub', 'base_url', fallback='http://127.0.0.1:1200')
    
    if config.has_section('x_accounts'):
        for display_name in config.options('x_accounts'):
            account_id = config.get('x_accounts', display_name).strip()
            if account_id:
                x_accounts[display_name] = f"{rsshub_base_url}/twitter/user/{account_id}"
    
    return x_accounts

def load_youtube_channels_from_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ YouTube é¢‘é“åˆ—è¡¨
    
    é…ç½®æ ¼å¼ï¼šæ˜¾ç¤ºåç§° = é¢‘é“ID (ä»¥UCå¼€å¤´)
    
    è¿”å›ï¼š
        dict: {æ˜¾ç¤ºåç§°: RSSåœ°å€}
    """
    youtube_channels = {}
    
    if config.has_section('youtube_channels'):
        for display_name in config.options('youtube_channels'):
            channel_id = config.get('youtube_channels', display_name).strip()
            if channel_id:
                youtube_channels[display_name] = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    
    return youtube_channels

# ================= é…ç½®åŒºåŸŸ =================
# è®¾ç½® RSSHub çš„è®¢é˜…æº (æŒ‰æ¥æºç±»å‹åˆ†ç»„)
# æç¤ºï¼šX (Twitter) å’Œ YouTube çš„è·¯ç”±å¯ä»¥åœ¨ https://docs.rsshub.app/ æ‰¾åˆ°
rss_sources = {
    "weixin": load_weixin_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å–å¾®ä¿¡å…¬ä¼—å·
    "X": load_x_accounts_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– X è´¦æˆ·
    "YouTube": load_youtube_channels_from_config(),  # ä»é…ç½®æ–‡ä»¶è¯»å– YouTube é¢‘é“
}

# ================= å†…å®¹å¢å¼ºæ¨¡å— =================
# ç”¨äºä»Xæ¨æ–‡ä¸­æå–åµŒå…¥é“¾æ¥å†…å®¹ï¼Œä»¥åŠä»YouTubeè§†é¢‘ä¸­æå–å­—å¹•
content_fetcher = ContentFetcher()
# ===========================================


def generate_post_markdown(post, domain):
    """ç”Ÿæˆå•ç¯‡æ–‡ç« çš„ Markdown å†…å®¹"""
    # ç”Ÿæˆæ˜Ÿçº§è¯„åˆ†æ˜¾ç¤º
    score = post.get('quality_score', 3)
    stars = 'â­' * score + 'â˜†' * (5 - score)
    
    lines = [
        f"# {post.get('event', 'æœªå‘½åäº‹ä»¶')}",
        "",
        f"- **æ—¥æœŸ**: {post.get('date', 'æœªçŸ¥æ—¥æœŸ')}",
        f"- **äº‹ä»¶åˆ†ç±»**: {post.get('category', 'æœªåˆ†ç±»')}",
        f"- **æ‰€å±é¢†åŸŸ**: {domain}",
        f"- **è´¨é‡è¯„åˆ†**: {stars} ({score}/5)",
        f"- **è¯„åˆ†ç†ç”±**: {post.get('quality_reason', 'æ— ')}",
        f"- **æ¥æº**: {post.get('source_name', 'æœªçŸ¥')}",
        f"- **åŸæ–‡é“¾æ¥**: {post.get('link', '')}",
        "",
        "## å…³é”®ä¿¡æ¯",
        post.get('key_info', ''),
        "",
        "## è¯¦ç»†å†…å®¹",
        post.get('detail', ''),
        "",
    ]
    
    if post.get('extra_content'):
        lines.extend(["â€‹## è¡¥å……å†…å®¹", post['extra_content'], ""])
    
    if post.get('extra_urls'):
        lines.append("## å¤–éƒ¨é“¾æ¥")
        lines.extend([f"- {url}" for url in post['extra_urls']])
        lines.append("")
    
    return "\n".join(lines)


# ================= è¾…åŠ©å‡½æ•° =================

def _parse_date(entry):
    """è§£æå¹¶æ ‡å‡†åŒ–æ—¶é—´"""
    if not hasattr(entry, 'published'): return None
    dt = date_parser.parse(entry.published)
    return dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt

def _enrich_x_content(content, title):
    """æå– X æ¨æ–‡çš„åµŒå…¥å†…å®¹"""
    try:
        enable_opt = config.getboolean('llm', 'enable_subtitle_optimization', fallback=False)
        embedded, extra_urls = content_fetcher.fetch_embedded_content(content, title=title, optimize_video=enable_opt)
        extra_content = ""
        if embedded:
            parts = [f"[{'åšå®¢' if i.content_type == 'blog' else 'è§†é¢‘å­—å¹•'}] {i.content}" 
                     for i in embedded if i.content]
            extra_content = "\n\n".join(parts)
        
        if embedded or extra_urls:
            t = (title or "æ— æ ‡é¢˜")
            t = t[:30] + "..." if len(t) > 30 else t
            logger.info(f"[{t}] åµŒå…¥: {len(embedded)}, å¤–é“¾: {len(extra_urls)}")
        return extra_content, extra_urls
    except Exception as e:
        logger.info(f"Xå†…å®¹æå–å¤±è´¥: {e}")
        return "", []

def _enrich_youtube_content(link, title, context=""):
    """æå– YouTube å­—å¹•
    
    å‚æ•°:
        link: è§†é¢‘é“¾æ¥
        title: è§†é¢‘æ ‡é¢˜
        context: ä¸Šä¸‹æ–‡ï¼ˆé€šå¸¸æ˜¯RSSæ‘˜è¦/æè¿°ï¼‰
    """
    try:
        # ä¼ é€’ title å’Œ context åˆ° fetchï¼Œcontext ç”¨ä½œè¡¥å……ä¿¡æ¯
        full_context = f"{title}\n{context}" if context else title
        # ä»é…ç½®è¯»å–æ˜¯å¦å¯ç”¨å­—å¹•ä¼˜åŒ–
        enable_opt = config.getboolean('llm', 'enable_subtitle_optimization', fallback=False)
        yt = content_fetcher.video_fetcher.fetch(link, context=full_context, title=title, optimize=enable_opt)
        if yt and yt.content:
            logger.info(f"æå–åˆ°å­—å¹•: {len(yt.content)} å­—ç¬¦")
            return yt.content
    except Exception as e:
        logger.info(f"å­—å¹•æå–å¤±è´¥: {e}")
    return ""

def _save_raw_backup(posts, source_type, name):
    """ä¿å­˜åŸå§‹æ•°æ®å¤‡ä»½"""
    if not posts: return
    try:
        raw_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
        os.makedirs(raw_dir, exist_ok=True)
        safe_name = "".join(c if c.isalnum() or c in '-_' else '_' for c in name)
        filename = f"{source_type}_{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(os.path.join(raw_dir, filename), 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.info(f"å¤‡ä»½å¤±è´¥: {e}")


def _enrich_single_post(post):
    """
    å¯¹å•ä¸ªå¸–å­è¿›è¡Œå†…å®¹å¢å¼ºï¼ˆç”¨äºå¹¶è¡Œæ‰§è¡Œï¼‰
    
    æ ¹æ® source_type æ‰§è¡Œç›¸åº”çš„å¢å¼ºæ“ä½œï¼š
    - X: æå–åµŒå…¥é“¾æ¥å†…å®¹
    - YouTube: æå–è§†é¢‘å­—å¹•
    
    è¿”å›:
        post: å¢å¼ºåçš„å¸–å­ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
    """
    source_type = post.get('source_type', '')
    title = post.get('title', '')
    
    try:
        if source_type == "X":
            content = post.get('content', '')
            extra_content, extra_urls = _enrich_x_content(content, title)
            post['extra_content'] = extra_content
            post['extra_urls'] = extra_urls
        elif source_type == "YouTube":
            link = post.get('link', '')
            content = post.get('content', '')
            extra_content = _enrich_youtube_content(link, title, content)
            post['extra_content'] = extra_content
    except Exception as e:
        t = title[:30] + "..." if len(title) > 30 else title
        logger.info(f"[{t}] å†…å®¹å¢å¼ºå¤±è´¥: {e}")
    
    return post


def enrich_posts_parallel(posts, max_workers=5):
    """
    æ‰¹é‡å¹¶è¡Œå¢å¼ºå¸–å­å†…å®¹
    
    å‚æ•°:
        posts: å¾…å¢å¼ºçš„å¸–å­åˆ—è¡¨
        max_workers: å¹¶å‘æ•°ï¼ˆé»˜è®¤ 5ï¼‰
    
    è¿”å›:
        å¢å¼ºåçš„å¸–å­åˆ—è¡¨
    """
    # ç­›é€‰éœ€è¦å¢å¼ºçš„å¸–å­ï¼ˆX å’Œ YouTubeï¼‰
    posts_to_enrich = [p for p in posts if p.get('source_type') in ('X', 'YouTube')]
    
    if not posts_to_enrich:
        logger.info("æ²¡æœ‰éœ€è¦å†…å®¹å¢å¼ºçš„å¸–å­")
        return posts
    
    logger.info(f"ğŸ”„ å¼€å§‹å¹¶è¡Œå†…å®¹å¢å¼ºï¼Œå…± {len(posts_to_enrich)} ç¯‡...")
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(_enrich_single_post, p): p for p in posts_to_enrich}
        
        completed = 0
        for future in as_completed(futures):
            completed += 1
            try:
                future.result()  # ç»“æœå·²åŸåœ°ä¿®æ”¹åˆ° post ä¸­
            except Exception as e:
                logger.error(f"å†…å®¹å¢å¼ºä»»åŠ¡å¼‚å¸¸: {e}")
            
            if completed % 5 == 0:
                logger.info(f"å†…å®¹å¢å¼ºè¿›åº¦: {completed}/{len(posts_to_enrich)}")
    
    elapsed = time.time() - start_time
    logger.info(f"âœ… å†…å®¹å¢å¼ºå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")
    
    return posts


def fetch_recent_posts(rss_url, days, source_type="æœªçŸ¥", name="", save_raw=True):
    """
    æŠ“å– RSS å¹¶ç­›é€‰æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹ï¼ˆä»…æŠ“å–åŸºç¡€æ•°æ®ï¼Œä¸åšå†…å®¹å¢å¼ºï¼‰
    
    å‚æ•°ï¼š
        rss_url: RSS æºåœ°å€
        days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å†…å®¹
        source_type: æ¥æºç±»å‹ï¼ˆå¾®ä¿¡å…¬ä¼—å·ã€X (Twitter)ã€YouTubeã€åšå®¢/æ–°é—»ç­‰ï¼‰
        name: æºåç§°
        save_raw: æ˜¯å¦ä¿å­˜åŸå§‹æ•°æ®ä¸º JSON å¤‡ä»½æ–‡ä»¶
    
    æ³¨æ„ï¼šå†…å®¹å¢å¼ºï¼ˆX åµŒå…¥é“¾æ¥ã€YouTube å­—å¹•ï¼‰å°†åœ¨åç»­é˜¶æ®µå¹¶è¡Œæ‰§è¡Œ
    """
    logger.info(f"æ­£åœ¨æŠ“å– [{source_type}] {name}: {rss_url} ...")
    try:
        feed = feedparser.parse(rss_url)
        
        # æ£€æŸ¥ RSS è§£ææ˜¯å¦å‡ºé”™
        if feed.bozo and not feed.entries:
            logger.info(f"RSS è§£æå¤±è´¥: {feed.bozo_exception}")
            return []
        
        recent_posts = []
        
        # è·å–å½“å‰æ—¶é—´ (å¸¦æ—¶åŒºæ„ŸçŸ¥ï¼Œé»˜è®¤ä¸º UTC ä»¥ä¾¿æ¯”è¾ƒ)
        now = datetime.now(timezone.utc)
        
        for entry in feed.entries:
            # 1. æ—¶é—´æ£€æŸ¥
            post_date = _parse_date(entry)
            if not post_date or (now - post_date).days > days:
                continue

            # 2. åŸºç¡€å†…å®¹æå–ï¼ˆä¸åšå†…å®¹å¢å¼ºï¼‰
            content = entry.get('content', '') or entry.get('description', '')

            logger.info(f"æ ‡é¢˜: {entry.title}")

            # å†…å®¹å¢å¼ºå­—æ®µç•™ç©ºï¼Œåç»­å¹¶è¡Œå¡«å……
            recent_posts.append({
                "title": entry.title,
                "date": post_date.strftime("%Y-%m-%d"),
                "link": entry.link,
                "rss_url": rss_url,
                "source_type": source_type,
                "source_name": name,
                "content": content,
                "extra_content": "",   # å»¶è¿Ÿå¡«å……
                "extra_urls": []       # å»¶è¿Ÿå¡«å……
            })
        
        # ä¿å­˜å¤‡ä»½
        if save_raw:
            _save_raw_backup(recent_posts, source_type, name)
                
        return recent_posts
    except Exception as e:
        logger.info(f"æŠ“å–å¤±è´¥: {e}")
        return []


# ================= ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    start_time = time.time()
    MAX_WORKERS = config.getint('crawler', 'organize_workers', fallback=5)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”¨äºè¿½è¸ªå·²åˆ›å»ºçš„é¢†åŸŸç›®å½• 
    # {domain: {'path': dir_path, 'name': dir_name, 'high': count, 'pending': count, 'excluded': count}}
    domain_dirs = {}
    
    def get_quality_tier(score):
        """æ ¹æ®è´¨é‡è¯„åˆ†è¿”å›å­ç›®å½•å"""
        if score >= 4:
            return "high"       # é«˜è´¨é‡ (4-5åˆ†)
        elif score >= 2:
            return "pending"    # å¾…å®š (2-3åˆ†)
        else:
            return "excluded"   # æ’é™¤ (1åˆ†)
    
    def get_domain_dir(domain):
        """è·å–é¢†åŸŸç›®å½•è·¯å¾„ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºï¼ˆåŒ…å« high/pending/excluded ä¸‰ä¸ªå­ç›®å½•ï¼‰"""
        if domain not in domain_dirs:
            safe_domain = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in domain)
            dir_name = f"{safe_domain}_{timestamp}"
            dir_path = os.path.join(output_dir, dir_name)
            
            # åˆ›å»ºé¢†åŸŸä¸»ç›®å½•å’Œä¸‰ä¸ªå­ç›®å½•
            for tier in ['high', 'pending', 'excluded']:
                os.makedirs(os.path.join(dir_path, tier), exist_ok=True)
            
            domain_dirs[domain] = {
                'path': dir_path, 
                'name': dir_name, 
                'high': 0, 
                'pending': 0, 
                'excluded': 0
            }
        return domain_dirs[domain]
    
    def write_post_file(result):
        """å°†å•ç¯‡æ–‡ç« å†™å…¥å¯¹åº”é¢†åŸŸçš„è´¨é‡åˆ†çº§ç›®å½•"""
        domain = result.get('domain', 'å…¶ä»–')
        event = result.get('event', 'æœªå‘½åäº‹ä»¶')
        date_str = result.get('date', 'æœªçŸ¥æ—¥æœŸ')
        quality_score = result.get('quality_score', 3)
        
        domain_info = get_domain_dir(domain)
        tier = get_quality_tier(quality_score)
        
        safe_event = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in event)[:50]
        filename = f"{safe_event}_{date_str}.md"
        filepath = os.path.join(domain_info['path'], tier, filename)
        
        md_content = generate_post_markdown(result, domain)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        domain_info[tier] += 1
    
    # 1. å‡†å¤‡æºåˆ—è¡¨
    sources_list = [
        (category, name, url) 
        for category, sources in rss_sources.items()
        for name, url in sources.items()
    ]
    
    ENRICH_WORKERS = config.getint('crawler', 'enrich_workers', fallback=5)
    
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(sources_list)} ä¸ªè®¢é˜…æº (ä¸²è¡ŒæŠ“å– -> å¹¶è¡Œå¢å¼º -> å¹¶è¡Œæ•´ç†)...")
    
    all_organized_posts = []
    
    # ========== é˜¶æ®µ 1: ä¸²è¡ŒæŠ“å–æ‰€æœ‰ RSS æº ==========
    all_posts = []
    for category, name, url in sources_list:
        posts = fetch_recent_posts(url, DAYS_LOOKBACK, source_type=category, name=name)
        if posts:
            logger.info(f"-> [{name}] è·å– {len(posts)} æ¡")
            all_posts.extend(posts)
    
    logger.info(f"å…±è·å– {len(all_posts)} ç¯‡æ–‡ç« ")
    
    # ========== é˜¶æ®µ 2: å¹¶è¡Œå†…å®¹å¢å¼º ==========
    enrich_posts_parallel(all_posts, max_workers=ENRICH_WORKERS)
    
    # ========== é˜¶æ®µ 3: å¹¶è¡Œ LLM æ•´ç† ==========
    logger.info(f"å¼€å§‹å¹¶è¡Œ LLM æ•´ç†...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # å‡†å¤‡ä»»åŠ¡ï¼ˆéœ€è¦ source_nameï¼‰
        futures = {
            executor.submit(organize_single_post, post, post.get('source_name', '')): post
            for post in all_posts
        }
        
        # 4. è·å–ç»“æœ & å³æ—¶å†™å…¥
        completed = 0
        for future in as_completed(futures):
            post = futures[future]
            completed += 1
            try:
                result = future.result()
                if result:
                    all_organized_posts.append(result)
                    write_post_file(result)  # å³æ—¶å†™å…¥
            except Exception as e:
                name = post.get('source_name', 'æœªçŸ¥')
                logger.error(f"âŒ [{name}] æ•´ç†å¤±è´¥: {e}")
            
            if completed % 10 == 0:
                logger.info(f"è¿›åº¦: {completed}/{len(futures)}")
    
    logger.info(f"æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œå…±è·å– {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹")
    
    # 5. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_high = sum(info['high'] for info in domain_dirs.values())
    total_pending = sum(info['pending'] for info in domain_dirs.values())
    total_excluded = sum(info['excluded'] for info in domain_dirs.values())
    
    # 6. ä¿å­˜æ‰¹æ¬¡æ¸…å•
    domain_report_dirs = {domain: info['name'] for domain, info in domain_dirs.items()}
    save_batch_manifest(
        output_dir=output_dir,
        batch_id=timestamp,
        domain_reports=domain_report_dirs,
        stats={
            "total_posts": len(all_organized_posts),
            "domain_count": len(domain_dirs),
            "quality_distribution": {
                "high": total_high,
                "pending": total_pending,
                "excluded": total_excluded
            }
        }
    )
    
    # æ‰“å°æ‰§è¡Œç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
    print("="*60)
    print(f"æ€»å…±å¤„ç†: {len(all_organized_posts)} æ¡æœ‰æ•ˆå†…å®¹")
    print(f"\nè´¨é‡åˆ†å¸ƒ:")
    print(f"  â­ é«˜è´¨é‡ (high):     {total_high} æ¡")
    print(f"  ğŸ”¶ å¾…å®š (pending):    {total_pending} æ¡")
    print(f"  â›” æ’é™¤ (excluded):   {total_excluded} æ¡")
    print(f"\né¢†åŸŸåˆ†å¸ƒ:")
    for domain, info in domain_dirs.items():
        total = info['high'] + info['pending'] + info['excluded']
        print(f"  - {domain}: {total} æ¡ (é«˜:{info['high']} / å¾…å®š:{info['pending']} / æ’é™¤:{info['excluded']})")
        logger.info(f"âœ… é¢†åŸŸ [{domain}] é«˜è´¨é‡:{info['high']} å¾…å®š:{info['pending']} æ’é™¤:{info['excluded']}")
    print(f"\nç”Ÿæˆç›®å½•:")
    for domain, info in domain_dirs.items():
        print(f"  - {info['name']}/")
        print(f"      â”œâ”€â”€ high/     ({info['high']} ä¸ªæ–‡ä»¶)")
        print(f"      â”œâ”€â”€ pending/  ({info['pending']} ä¸ªæ–‡ä»¶)")
        print(f"      â””â”€â”€ excluded/ ({info['excluded']} ä¸ªæ–‡ä»¶)")
    
    elapsed_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"âœ… æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print("="*60)
